{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname('./..'))))\n",
    "\n",
    "from Data.Dataset import FinTextDataset\n",
    "from Data.DataLoader import FinTextDataLoader\n",
    "from Model.MainModel import FinTextModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "def divide_chunks(lt, n):\n",
    "     \n",
    "    for i in range(0, len(lt), n):\n",
    "        yield lt[i:i + n]\n",
    "\n",
    "def to(tensor, device):\n",
    "    tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'ElectraTokenizer'.\n",
      "Some weights of the model checkpoint at beomi/KcELECTRA-base-v2022 were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/bob/문서/FinText/Data/Dataset.py:114: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedded_matrix = torch.tensor(embedded_matrix[0][0]).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "community_tensor\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'divide_chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_pickle(\u001b[39m'\u001b[39m\u001b[39m../data-dir/data-df.pkl\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m dataset \u001b[39m=\u001b[39m FinTextDataset(df)\n",
      "File \u001b[0;32m~/문서/FinText/Data/Dataset.py:178\u001b[0m, in \u001b[0;36mFinTextDataset.__init__\u001b[0;34m(self, df, **config)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[39mfor\u001b[39;00m i, row \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(total_row):\n\u001b[1;32m    177\u001b[0m         total_row[i] \u001b[39m=\u001b[39m row\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 178\u001b[0m     feature_dict[name] \u001b[39m=\u001b[39m make_chunk_and_stack(total_row)\n\u001b[1;32m    180\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(feature_dict)\n\u001b[1;32m    181\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\n\u001b[1;32m    182\u001b[0m     pd\u001b[39m.\u001b[39mget_dummies(df[\u001b[39m\"\u001b[39m\u001b[39mLabel\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mvalues[\u001b[39m0\u001b[39m : \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39mbundle_size\u001b[39m\u001b[39m\"\u001b[39m]]\n\u001b[1;32m    183\u001b[0m )\n",
      "File \u001b[0;32m~/문서/FinText/Data/Dataset.py:168\u001b[0m, in \u001b[0;36mFinTextDataset.__init__.<locals>.make_chunk_and_stack\u001b[0;34m(data_lt)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_chunk_and_stack\u001b[39m(data_lt):\n\u001b[1;32m    167\u001b[0m     row_lt \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 168\u001b[0m     \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m divide_chunks(data_lt, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39mbundle_size\u001b[39m\u001b[39m\"\u001b[39m]):\n\u001b[1;32m    169\u001b[0m         row_lt\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mstack(row))\n\u001b[1;32m    171\u001b[0m     \u001b[39mreturn\u001b[39;00m row_lt\n",
      "\u001b[0;31mNameError\u001b[0m: name 'divide_chunks' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle('../data-dir/data-df.pkl')\n",
    "dataset = FinTextDataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
